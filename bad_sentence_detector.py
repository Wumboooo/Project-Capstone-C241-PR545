# -*- coding: utf-8 -*-
"""Bad Sentence Detector.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10tlm-jwa2M1CNYEiMX_D-kEnh685FKxM

# Packages Preparation
"""

!pip install Sastrawi
!pip install wordcloud==1.8.1

# Commented out IPython magic to ensure Python compatibility.
import string
import re
import pathlib
import json
import pickle

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')

from wordcloud import WordCloud, STOPWORDS

import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
# %matplotlib inline

import seaborn as sns
sns.set_theme(style="whitegrid")

from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import models, regularizers, layers, optimizers, losses, initializers
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

"""# Import Data

https://www.kaggle.com/datasets/ilhamfp31/indonesian-abusive-and-hate-speech-twitter-text
"""

from google.colab import files
files.upload()

!rm -r ~/.kaggle
!mkdir ~/.kaggle
!mv ./kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d ilhamfp31/indonesian-abusive-and-hate-speech-twitter-text
!unzip /content/indonesian-abusive-and-hate-speech-twitter-text.zip -d /content/

"""# Data Preparation"""

df = pd.read_csv('/content/data.csv', encoding='latin-1')
df.head()

df.info()

"""# Data Cleansing

## Duplicated Data

Jumlah data yang duplikat.
"""

duplicated_row = df.duplicated(subset=['Tweet'])
duplicated_row.sum()

"""Hapus data duplikat yang duplikat."""

df.drop_duplicates(subset=['Tweet'], inplace=True)
df.shape

print('Jumlah missing value (null)')
print(df.isnull().sum())

"""# Data Preprocessing

Terdapat 5 tahap preprocessing yang dilakukan pada data teks disini, yaitu:
1. case folding
2. menghapus karakter yang tidak penting (URL, username, dll)
3. menghapus tanda baca
4. menghapus stopword
5. mengubah kata-kata alay menjadi lebih dapat dibaca
"""

alay_df = pd.read_csv('/content/new_kamusalay.csv',
                        encoding = 'latin-1',
                        header = None)

alay_df.rename(columns={0: 'original',
                        1: 'replacement'},
               inplace = True)

alay_dict_map = dict(zip(alay_df['original'], alay_df['replacement']))

def normalize_alay(text):
    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])

def convert_lower_case(text):
  return text.lower()

def remove_stop_words(text):
  stop_words = stopwords.words('indonesian')
  words = word_tokenize(str(text))
  new_text = ""
  for w in words:
    if w not in stop_words and len(w) > 1:
      new_text = new_text + " " + w
  return new_text

def remove_unnecessary_char(text):
  text = re.sub('\n',' ',text) # Remove every '\n'
  text = re.sub('rt',' ',text) # Remove every retweet symbol
  text = re.sub('user',' ',text) # Remove every username
  text = re.sub('((www\.[^\s]+)|(https?://[^\s]+)|(http?://[^\s]+))',' ',text) # Remove every URL
  text = re.sub('  +', ' ', text) # Remove extra spaces

  text = re.sub('x9f', ' ', text)
  text = re.sub('x98', ' ', text)
  text = re.sub('xf0', ' ', text)

  text = re.sub(' ya ', ' ', text)
  text = re.sub('x82', ' ', text)
  text = re.sub('uniform', ' ', text)
  text = re.sub('resource', ' ', text)

  text = re.sub('xe2', ' ', text)
  text = re.sub('x80', ' ', text)
  text = re.sub('x91', ' ', text)
  text = re.sub('x8c', ' ', text)

  text = re.sub('locator', ' ', text)
  return text

def remove_punctuation(text):
  symbols = string.punctuation
  for i in range(len(symbols)):
    text = text.replace(symbols[i], ' ')
    text = text.replace("  ", " ")
  text = text.replace(',', '')
  return text

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

def stemming(text):
  factory = StemmerFactory()
  stemmer = factory.create_stemmer()

  tokens = word_tokenize(str(text))
  new_text = ""
  for w in tokens:
    new_text = new_text + " " + stemmer.stem(w)
  return new_text

counter = 0

def preprocess(text, verbose=0):
  global counter

  text = convert_lower_case(text)
  text = remove_unnecessary_char(text)
  text = remove_punctuation(text)
  text = remove_stop_words(text)
  text = normalize_alay(text)
  # text = stemming(text)
  text = remove_stop_words(text)
  text = remove_punctuation(text)

  counter += 1
  if (counter % 10 == 0) and (verbose == 1):
    print(f"\r{counter}", end="")
  return text

df_clean = df.copy()

df_clean['Tweet'] = df.apply(lambda x: preprocess(x['Tweet'], verbose=1),
                             axis=1)

print()
print(df_clean['Tweet'].iloc[45])

"""# Exploratory Data Analysis

## Hate Speech
"""

hs = df_clean['HS'].value_counts()
hs.rename(index={0: "bukan hate speech",
                 1: "hate speech"},
          inplace=True)

colors = sns.color_palette('pastel')[0:5]

plt.pie(hs.values,
        labels = hs.index,
        colors = colors,
        autopct = '%1.1f%%',
        startangle = 90,
        explode = (0.1, 0))
plt.title('Tweet hate speech')
plt.show()

hs_category = df.iloc[:, 3:].sum()

plt.barh(y = hs_category.sort_values().index,
         width = hs_category.sort_values().values,
         color = 'grey')
plt.title("Kategori hate speech")

"""### Word Freq

Gabungkan semua teks tiap kategori hate speech
"""

hscat_text = dict()

for label in df_clean.columns[3:]:
  joined_text = str()
  for tweet in df_clean.loc[df_clean[label]==1]['Tweet']:
    joined_text += preprocess(tweet)
  hscat_text[label] = joined_text

"""Generate wordcloud"""

wc_dict = dict()

for label in hscat_text:
  wc_dict[label] = WordCloud(
      background_color='white',
      max_words=2000
  )

  wc_dict[label].generate(hscat_text[label])

"""Visualisasi wordcloud"""

fig, ax = plt.subplots(5, 2, figsize=(14,20))

for i, label in enumerate(df_clean.columns[3:]):
  row = int(i/2)
  col = i % 2

  ax[row, col].imshow(wc_dict[label])
  ax[row, col].set_title(label)
  ax[row, col].set_xticks([])
  ax[row, col].set_yticks([])

fig, ax = plt.subplots(5, 2, figsize=(12,20))

for i, label in enumerate(hscat_text):
  freq_dist = nltk.FreqDist(nltk.tokenize.word_tokenize(hscat_text[label]))

  df_cat = pd.DataFrame(list(freq_dist.items()), columns=["Kata", "Frekuensi"])
  df_cat = df_cat.sort_values(by=["Frekuensi"], ascending=False).head(10)

  row = int(i/2)
  col = i % 2

  ax[row, col].barh(y = sorted(df_cat['Kata']),
                    width = sorted(df_cat['Frekuensi']),
                    color = 'grey')
  ax[row, col].set_title(label)

plt.tight_layout()

"""## Not Hate Speech

### Word Freq

Gabungkat text antara teks hate speech dan teks bukan hate speech
"""

hscat_text = dict()

for label in [0, 1]:
  joined_text = str()
  for tweet in df_clean.loc[df_clean['HS']==label]['Tweet']:
    joined_text += preprocess(tweet)
  if label == 0:
    label = "Not HS"
  else:
    label = "HS"
  hscat_text[label] = joined_text

"""Generate word cloud"""

wc_dict = dict()

for label in hscat_text:
  wc_dict[label] = WordCloud(
      background_color='white',
      max_words=2000
  )

  wc_dict[label].generate(hscat_text[label])

"""Visualisasi word cloud"""

fig, ax = plt.subplots(1, 2, figsize=(14,14))

for i, label in enumerate(hscat_text):
  ax[i].imshow(wc_dict[label])
  ax[i].set_title(label)
  ax[i].set_xticks([])
  ax[i].set_yticks([])

fig, ax = plt.subplots(1, 2, figsize=(12,4))

for i, label in enumerate(hscat_text):
  freq_dist = nltk.FreqDist(nltk.tokenize.word_tokenize(hscat_text[label]))
  df_cat = pd.DataFrame(list(freq_dist.items()), columns=["Kata", "Frekuensi"])
  df_cat = df_cat.sort_values(by=["Frekuensi"], ascending=False).head(10)

  ax[i].barh(y = sorted(df_cat['Kata']),
             width = sorted(df_cat['Frekuensi']),
             color = 'grey')
  ax[i].set_title(label)

plt.tight_layout()

"""## Abusive"""

abusive = df_clean['Abusive'].value_counts()
abusive.rename(index={0: "Not abusive",
                      1: "Abusive"},
               inplace=True)

plt.pie(abusive.values,
        labels = abusive.index,
        colors = colors,
        autopct = '%1.1f%%',
        startangle = 90,
        explode = (0.1, 0))
plt.title('Abusive and not abusive tweet')
plt.show()

abusive = df_clean[df_clean['Abusive'] == 1]['HS'].value_counts()
abusive.rename(index={0: "Not hate speech",
                      1: "Hate speech"},
               inplace=True)

plt.pie(abusive.values,
        labels = abusive.index,
        colors = colors,
        autopct = '%1.1f%%',
        startangle = 90)
plt.title('Abusive with hate speech and abusive but not hate speech tweet')
plt.show()

abusive = df_clean[df_clean['Abusive'] == 1].iloc[:, 1:].sum().drop(['Abusive',
                                                                     'HS'])
plt.barh(y = abusive.sort_values().index,
         width = abusive.sort_values().values,
         color = 'grey')
plt.title('Abusive tweet')

"""### Word Freq

Gabungkan teks
"""

abusive_text = dict()

for label in [0, 1]:
  joined_text = str()
  for tweet in df_clean.loc[df_clean['Abusive']==label]['Tweet']:
    joined_text += preprocess(tweet)
  if label == 0:
    label = "Not Abusive"
  else:
    label = "Abusive"
  abusive_text[label] = joined_text

"""Generate word cloud"""

wc_dict = dict()

for label in abusive_text:
  wc_dict[label] = WordCloud(
      background_color='white',
      max_words=2000
  )

  wc_dict[label].generate(abusive_text[label])

"""Visualisasi Word Cloud"""

fig, ax = plt.subplots(1, 2, figsize=(14,14))

for i, label in enumerate(abusive_text):
  ax[i].imshow(wc_dict[label])
  ax[i].set_title(label)
  ax[i].set_xticks([])
  ax[i].set_yticks([])

fig, ax = plt.subplots(1, 2, figsize=(14,4))

for i, label in enumerate(abusive_text):
  freq_dist = nltk.FreqDist(nltk.tokenize.word_tokenize(abusive_text[label]))

  df_cat = pd.DataFrame(list(freq_dist.items()), columns=["Kata", "Frekuensi"])
  df_cat = df_cat.sort_values(by=["Frekuensi"], ascending=False).head(10)

  ax[i].barh(y = sorted(df_cat['Kata']),
             width = sorted(df_cat['Frekuensi']),
             color = 'grey')
  ax[i].set_title(label)

plt.tight_layout()

"""## Correlation"""

corr = df_clean.drop(['Tweet', 'HS', 'Abusive', 'HS_Other'],
                     axis=1).corr().abs()
mask = np.triu(corr)

plt.figure(figsize=(14,8))
sns.heatmap(corr, annot=True, mask=mask, fmt='.2f')
plt.title("Correlation")
plt.xticks(rotation=30)

"""# Sentiment Analysis

## Label Preparation
"""

df_sentiment = df_clean.copy().drop(df_clean.columns[3:],
                                    axis = 1)

def sentiment(hs, abusive):
  if (hs == 1) and (abusive == 1):
    return 'HS_Abusive'
  elif (hs == 1) and (abusive == 0):
    return 'HS_NotAbusive'
  elif (hs == 0) and (abusive == 1):
    return 'NotHS_Abusive'
  else:
    return 'NotHS_NotAbusive'

df_sentiment['sentiment'] = df_sentiment.apply(lambda x: sentiment(x['HS'],
                                                                   x['Abusive']),
                                               axis=1)

df_sentiment.drop(['HS', 'Abusive'], axis=1, inplace=True)

df_sentiment.head()

df_label = pd.get_dummies(df_sentiment['sentiment'])
df_sentiment.drop('sentiment', axis=1, inplace=True)
df_sentiment = pd.concat([df_sentiment['Tweet'], df_label], axis=1)

df_sentiment.head()

df_sentiment.sum()[1:].plot(kind='barh', color='grey',
                            title='Jumlah tweet tiap kategori')

tweet = df_sentiment['Tweet'].values
labels = df_sentiment.iloc[:, 1:].values

print(">>> Teks:")
print(tweet[:2])
print()
print(">>> Label:")
print(labels[:2])

labels_list = df_sentiment.columns[1:]
labels_list

"""## Data Splitting"""

x_train, x_val, y_train, y_val = train_test_split(tweet,
                                                  labels,
                                                  test_size=0.3,
                                                  random_state=45)

x_val, x_test, y_val, y_test = train_test_split(x_val, y_val,
                                                test_size=0.5,
                                                random_state=53)

"""## Tokenizing

Mengubah data teks yang awalnya berbentuk kalimat menjadi pecahan kata-kata atau token-token.

* Maksimal kosakata yang diambil untuk tokenizing adalah sebanyak 15000 berdasarkan kata yang paling sering muncul.
* Maksimal panjang array dalam satu data diatur sebanyak 1000.
* Jika jumlah kata dalam suatu data kurang dari 1000 kata, maka dalam array hasil tokenisasi, sisa nilai diakhirnya akan diatur menjadi 0.
"""

NUM_WORDS = 10000

tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token='')

tokenizer.fit_on_texts(x_train)

seq_train = tokenizer.texts_to_sequences(x_train)
seq_val = tokenizer.texts_to_sequences(x_val)
seq_test = tokenizer.texts_to_sequences(x_test)

x_train = pad_sequences(seq_train, padding='post',
                        maxlen=1000, truncating='post')
x_val = pad_sequences(seq_val, padding='post',
                      maxlen=1000, truncating='post')
x_test = pad_sequences(seq_test, padding='post',
                       maxlen=1000, truncating='post')

print(len(tokenizer.word_index))
print(x_train.max())
print(len(x_val[0]))

"""Simpan tokenizer"""

with open('/content/drive/MyDrive/Colab Notebooks/Data Science Trainee JSC/tokenizer.pickle',
          'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

"""## Modelling

Model yang akan digunakan adalah Bidirectional GRU dengan konfigurasi sebagai berikut:

* 1 BiGRU Layer dengan jumlah hidden state 32 dan dropout 0,5
* 1 Dense Layer dengan jumlah hidden state 16 dan regularisasi l2 0,5 serta fungsi aktivasi relu
* 1 Dropout Layer 0,5
* Output Layer dengan fungsi aktivasi softmax
* Optimizer Adam
* Learning rate = 0,001
* Epochs = 45, namun jika akurasi validasi telah mencapai lebih dari 90%, proses pelatihan akan langsung berhenti
* Batch size = 256
"""

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

import datetime

# Clear any logs from previous runs
!rm -rf ./logs/

highest_acc_train = [0,0]
highest_acc_test = [0,0]

class myCallback(keras.callbacks.Callback):

    def on_epoch_end(self, epoch, logs={}):
        global highest_acc_train, highest_acc_test

        target_acc = 0.9

        if logs.get('acc') > target_acc and logs.get('val_acc') > target_acc:
            print("\nThe accuracy has reached >", target_acc)
            self.model.stop_training = True
        if logs.get('acc') > highest_acc_train[0]:
            highest_acc_train[0] = logs.get('acc')
            highest_acc_train[1] = epoch+1
        if logs.get('val_acc') > highest_acc_test[0]:
            highest_acc_test[0] = logs.get('val_acc')
            highest_acc_test[1] = epoch+1

early_stopping = keras.callbacks.EarlyStopping(patience=5,
                                               monitor='val_loss',
                                               min_delta=0.01,
                                               verbose=1)

log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,
                                                      histogram_freq=1)

callbacks = myCallback()

model = models.Sequential([
    layers.Embedding(input_dim=NUM_WORDS, output_dim=16),
    layers.Bidirectional(layers.LSTM(32, dropout=0.5)),
    layers.Dense(16, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(len(labels_list), activation='softmax')
])

# model.summary()

model.compile(loss=losses.CategoricalCrossentropy(),
              optimizer=optimizers.Adam(learning_rate=0.001),
              metrics=['acc'])

hist = model.fit(x_train, y_train,
                 epochs=45,
                 batch_size=256,
                 validation_data=(x_val, y_val),
                 validation_steps=5,
                 verbose=2,
                 callbacks=[callbacks, early_stopping,
                            tensorboard_callback])

print("\nHighest training accuracy: %.4f | epoch: %i"%(highest_acc_train[0],
                                                       highest_acc_train[1]))
print("Highest validation accuracy: %.4f | epoch: %i"%(highest_acc_test[0],
                                                       highest_acc_test[1]))

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/fit

fig, ax = plt.subplots(1, 2, figsize = (8,4))

ax[0].plot(hist.history['acc'], label = 'train_acc')
ax[0].plot(hist.history['val_acc'], label = 'val_acc')
ax[0].set_title('Accuracy')
ax[0].legend()
ax[0].set_ylim(0, 1)

ax[1].plot(hist.history['loss'], label = 'train_loss')
ax[1].plot(hist.history['val_loss'], label = 'val_loss')
ax[1].legend()
ax[1].set_title('Loss')
#ax[1].set_ylim([0,max(hist.history['loss'] + hist.history['val_loss'])])
ax[1].set_ylim([0,5])

fig.tight_layout()
plt.show()

from sklearn.metrics import confusion_matrix

# model = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/Data Science Trainee JSC/model.h5')

# Melakukan prediksi pada data uji
y_pred = model.predict(x_test)
y_pred = np.argmax(y_pred, axis=1)  # Mengubah hasil prediksi menjadi label kelas

# Mengumpulkan label sebenarnya dari data uji
y_true = np.argmax(y_test, axis=1)

# Menghitung confusion matrix
cm = confusion_matrix(y_true, y_pred)
print(cm)

print(labels_list)

sns.heatmap(cm, annot=True, cmap='Blues', fmt='.0f')

plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.xticks(range(4), labels_list, rotation=45)
plt.yticks(range(4), labels_list, rotation=0)
plt.show()

from sklearn.metrics import classification_report, accuracy_score

print(classification_report(y_true, y_pred))

print(labels_list)

print(accuracy_score(y_true, y_pred))

"""## Saved Model"""

!pip install tensorflowjs

model.save("/content/drive/MyDrive/Colab Notebooks/Data Science Trainee JSC/model.h5")

!tensorflowjs_converter --input_format=keras model.h5 tfjs_model

from google.colab import drive
drive.mount('/content/drive')

"""# Let's Play

## Package Prepraration
"""

!pip install Sastrawi

import string
import re
import pickle

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')

import numpy as np
import pandas as pd

import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences

alay_df = pd.read_csv('/content/new_kamusalay.csv',
                        encoding = 'latin-1',
                        header = None)

alay_df.rename(columns={0: 'original',
                        1: 'replacement'},
               inplace = True)

alay_dict_map = dict(zip(alay_df['original'], alay_df['replacement']))

def normalize_alay(text):
  return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])

def convert_lower_case(text):
  return text.lower()

def remove_stop_words(text):
  stop_words = stopwords.words('indonesian')
  words = word_tokenize(str(text))
  new_text = ""
  for w in words:
    if w not in stop_words and len(w) > 1:
      new_text = new_text + " " + w
  return new_text

def remove_unnecessary_char(text):
  text = re.sub('\n',' ',text) # Remove every '\n'
  text = re.sub('rt',' ',text) # Remove every retweet symbol
  text = re.sub('user',' ',text) # Remove every username
  text = re.sub('((www\.[^\s]+)|(https?://[^\s]+)|(http?://[^\s]+))',' ',text) # Remove every URL
  text = re.sub('  +', ' ', text) # Remove extra spaces

  text = re.sub('x9f', ' ', text)
  text = re.sub('x98', ' ', text)
  text = re.sub('xf0', ' ', text)

  text = re.sub(' ya ', ' ', text)
  text = re.sub('x82', ' ', text)
  text = re.sub('uniform', ' ', text)
  text = re.sub('resource', ' ', text)

  text = re.sub('xe2', ' ', text)
  text = re.sub('x80', ' ', text)
  text = re.sub('x91', ' ', text)
  text = re.sub('x8c', ' ', text)

  text = re.sub('locator', ' ', text)
  return text

def remove_punctuation(text):
  symbols = string.punctuation
  for i in range(len(symbols)):
    text = text.replace(symbols[i], ' ')
    text = text.replace("  ", " ")
  text = text.replace(',', '')
  return text

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

def stemming(text):
  factory = StemmerFactory()
  stemmer = factory.create_stemmer()

  tokens = word_tokenize(str(text))
  new_text = ""
  for w in tokens:
    new_text = new_text + " " + stemmer.stem(w)
  return new_text

counter = 0

def preprocess(text, verbose=0):
  global counter

  text = convert_lower_case(text)
  text = remove_unnecessary_char(text)
  text = remove_punctuation(text)
  text = remove_stop_words(text)
  text = normalize_alay(text)
  # text = stemming(text)
  text = remove_stop_words(text)
  text = remove_punctuation(text)

  counter += 1
  if (counter % 10 == 0) and (verbose == 1):
    print(f"\r{counter}", end="")
  return text

with open('/content/drive/MyDrive/Colab Notebooks/tokenizer.pickle',
          'rb') as handle:
    tokenizer = pickle.load(handle)

model = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/Data Science Trainee JSC/model.h5')

"""## Write a tweet"""

whats_happening = 'tadi belu buku mahal banget ya' #@param {type: 'string'}

preprocess_tweet = preprocess(whats_happening, verbose=0)

seq_tweet = tokenizer.texts_to_sequences([preprocess_tweet])

the_tweet = pad_sequences(seq_tweet, padding='post',
                          maxlen=1000, truncating='post')

prediction = model.predict(the_tweet, verbose=0)

classes = np.argmax(prediction, axis = 1)

dict_classes = {
    0: 'VERY NEGATIVE. Hate Speech and Abusive Tweet',
    1: 'NEGATIVE. Hate Speech but NOT Abusive Tweet',
    2: 'NETRAL. NOT Hate Speech but Abusive Tweet',
    3: 'POSITIVE. NOT Hate Speech and NOT Abusive Tweet'
}

print('\n\"' + whats_happening + '\"\n')
print("This tweet is:\n   ", dict_classes[classes[0]])